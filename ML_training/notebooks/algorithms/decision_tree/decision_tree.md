# √Årvore de Decis√£o

## üìå Descri√ß√£o

A **√Årvore de Decis√£o** √© um algoritmo de aprendizado supervisionado usado tanto para **classifica√ß√£o** quanto para **regress√£o**. Sua principal caracter√≠stica √© sua estrutura ramificada que, mapeando as possiblidades, na parte de classifica√ß√£o, consegue decidir a resposta para determinada condi√ß√£o.

![Exemplo de √Årvore de Decis√£o](https://colaborae.com.br/wp-content/uploads/2023/07/arvore.png)

Sua estrutura √© formada por **n√≥s**: Ele possui o n√≥ de origem, seus ramos, n√≥s intermedi√°rios e seus filhos. Sendo o n√≥ final, **ou folha**, o respons√°vel pela decis√£o tomada.

> üß† Intui√ß√£o: Pensa que voc√™ √© uma pessoa tentando decidir se deve levar um guarda-chuva antes de sair de casa.

Voc√™ vai tomar decis√µes com base em perguntas simples, tipo:

    Est√° nublado?

        Se sim, v√° para a pr√≥xima pergunta.

        Se n√£o, provavelmente n√£o vai chover, ent√£o n√£o leva o guarda-chuva.

    A previs√£o diz que vai chover?

        Se sim, ent√£o leve o guarda-chuva.

        Se n√£o, n√£o precisa levar.


![Intui√ß√£o](https://www.hashtagtreinamentos.com/wp-content/uploads/2022/11/Arvore-de-Decisao-1.png)

---

## 1. üß≠ Coeficinetes

1. **Gini (impureza de Gini)**

    Mede a probabilidade de um item ser classificado errado se for escolhido aleatoriamente.

    Varia de 0 (puro, s√≥ uma classe) at√© um valor m√°ximo (quanto mais bagun√ßa, mais alto).

    $$
    Gini = 1 - \sum p_i^2
    $$

    Onde pipi‚Äã √© a propor√ß√£o de cada classe no grupo.

üß† Interpreta√ß√£o intuitiva: qu√£o "misturado" est√° o grupo. Quanto mais misturado (ex: metade sim, metade n√£o), maior a impureza.

2. **Entropia (ou ganho de informa√ß√£o)**

    Vem da teoria da informa√ß√£o.

    Mede o grau de incerteza ou impureza.

    $$
    Entropia = - \sum p_i \log_2(p_i)
    $$

    O ganho de informa√ß√£o √© quanto a entropia diminui ao fazer uma divis√£o.

üß† Intui√ß√£o: quanto mais ‚Äúbagun√ßado‚Äù o grupo, maior a entropia. A gente quer divis√µes que deixem os grupos mais ‚Äúpuros‚Äù.

3. **Redu√ß√£o de vari√¢ncia (usado em √°rvores de regress√£o)**

    Em problemas de regress√£o (prever n√∫meros, n√£o classes), usa-se a vari√¢ncia como medida de qualidade da divis√£o.

    A √°rvore tenta fazer cortes que reduzam a vari√¢ncia dos valores nos grupos.

üß† Intui√ß√£o: se voc√™ separar os dados e o valor das sa√≠das ficar mais parecido dentro de cada grupo, √© uma boa divis√£o.

---

2. ‚úÇÔ∏è Podagem (Pruning)

Durante o treinamento, uma √°rvore de decis√£o pode crescer demais, criando divis√µes muito espec√≠ficas para os dados de treino. Isso pode causar overfitting, ou seja, o modelo aprende os "ru√≠dos" dos dados ao inv√©s dos padr√µes reais.

A podagem serve para controlar o tamanho da √°rvore, removendo partes desnecess√°rias e tornando o modelo mais simples e eficiente.
üî∏ Tipos de podagem:
üîπ Poda Pr√©via (Pr√©-podamento)

Acontece durante a constru√ß√£o da √°rvore. Aqui, colocamos limites para impedir que ela cres√ßa demais:

    max_depth: profundidade m√°xima da √°rvore

    min_samples_split: n√∫mero m√≠nimo de amostras para dividir um n√≥

    min_samples_leaf: n√∫mero m√≠nimo de amostras em uma folha

Aqui a √°rvore √© constru√≠da completamente, e depois s√£o removidos os ramos que n√£o contribuem muito para a performance, com base em uma m√©trica de custo/complexidade.

    Em scikit-learn, usamos o par√¢metro ccp_alpha (pruning de custo-complexidade).

modelos_podados = [DecisionTreeClassifier(ccp_alpha=alpha).fit(X_train, y_train) for alpha in ccp_alphas]

üß† Intui√ß√£o:

Pensa numa √°rvore real cheia de galhos. Se voc√™ deixar todos eles crescerem, alguns v√£o ficar fracos, secos ou in√∫teis. Podando esses galhos, a √°rvore fica mais forte, mais saud√°vel e mais eficiente. O mesmo vale para a √°rvore de decis√£o.