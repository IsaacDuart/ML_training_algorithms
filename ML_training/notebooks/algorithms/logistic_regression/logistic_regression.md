# Regress√£o Log√≠stica

## üìå Descri√ß√£o

A **Regress√£o Log√≠stica** √© um algoritmo de aprendizado supervisionado amplamente utilizado para tarefas de **classifica√ß√£o**, especialmente em problemas bin√°rios (ex: doen√ßa vs. saud√°vel, spam vs. n√£o-spam).

Diferente da regress√£o linear, seu objetivo n√£o √© prever valores cont√≠nuos, mas sim **estimar a probabilidade** de uma amostra pertencer a uma determinada classe. Essa probabilidade √© obtida aplicando a fun√ß√£o **sigmoide (log√≠stica)** sobre uma combina√ß√£o linear das vari√°veis de entrada.

> üìà Por exemplo: se o modelo retorna uma probabilidade de 0.82 para a classe "positivo", isso pode ser interpretado como uma **alta confian√ßa** de que a inst√¢ncia pertence √†quela classe.

![Exemplo de Regress√£o Log√≠stica](https://brains.dev/wp-content/uploads/2023/01/log_reg8.png)

---

## 1. ‚öôÔ∏è Limiar de Decis√£o

### üîπ Defini√ß√£o

A sa√≠da da regress√£o log√≠stica √© uma probabilidade entre 0 e 1. Para transformar essa probabilidade em uma **decis√£o de classe**, utiliza-se um **limiar de decis√£o (threshold)**.  
O valor padr√£o mais comum √© **0.5**, ou seja:
- Se `p >= 0.5`, a amostra √© classificada como **classe positiva**;
- Caso contr√°rio, como **classe negativa**.

Esse limiar pode ser ajustado dependendo da aplica√ß√£o. Por exemplo, em diagn√≥sticos m√©dicos, pode ser interessante **reduzir o limiar** para detectar mais casos positivos, mesmo ao custo de mais falsos positivos.

![Exemplo de Limiar de Decis√£o](https://d1.awsstatic.com/S-curve.36de3c694cafe97ef4e391ed26a5cb0b357f6316.png)

---

## 2. üîß Hiperpar√¢metros Importantes

A regress√£o log√≠stica possui diversos **hiperpar√¢metros** que impactam diretamente sua performance e converg√™ncia:

### üîπ `penalty`

Controla o tipo de **regulariza√ß√£o** usada para evitar overfitting:
- `"l2"` (Ridge): penaliza grandes pesos, preferida na maioria dos casos.
- `"l1"` (Lasso): for√ßa alguns coeficientes a zero (sparse), √∫til para sele√ß√£o de atributos.
- `"elasticnet"`: combina√ß√£o entre L1 e L2.
- `"none"`: sem regulariza√ß√£o (raro, e pode causar overfitting).

### üîπ `C`

√â o **inverso da for√ßa de regulariza√ß√£o**.  
Valores menores de `C` significam **maior regulariza√ß√£o**.  
Por exemplo:
- `C = 1.0` (valor padr√£o) √© um bom ponto de partida.
- `C < 1` aumenta a penaliza√ß√£o, o que reduz o risco de overfitting.
- `C > 1` reduz a penaliza√ß√£o, permitindo ajustes mais livres dos pesos.

### üîπ `solver`

Define o algoritmo usado para otimizar a fun√ß√£o de custo:

| Solver      | Suporta L1? | Suporta L2? | Recomendado para: |
|-------------|-------------|-------------|--------------------|
| `liblinear` | ‚úÖ           | ‚úÖ           | Datasets pequenos, L1 ou L2 |
| `saga`      | ‚úÖ           | ‚úÖ           | Datasets grandes, elasticnet |
| `lbfgs`     | ‚ùå           | ‚úÖ           | Multiclasse, datasets grandes |
| `newton-cg` | ‚ùå           | ‚úÖ           | Multiclasse |
| `sag`       | ‚ùå           | ‚úÖ           | Datasets grandes e esparsos |

> ‚ö†Ô∏è Importante: nem todos os solvers suportam todas as penaliza√ß√µes.

### üîπ `max_iter`

N√∫mero m√°ximo de itera√ß√µes para converg√™ncia.  
√ötil aumentar esse valor se o modelo estiver emitindo avisos de que **n√£o convergiu**.

---

## 3. üìä Avalia√ß√£o do Modelo

Como a sa√≠da √© uma probabilidade, a Regress√£o Log√≠stica permite avaliar bem a performance do modelo em tarefas de classifica√ß√£o, utilizando m√©tricas como:

- **Acur√°cia**
- **Precis√£o / Recall**
- **F1-score**
- **Curva ROC e AUC**
- **Matriz de Confus√£o**

Al√©m disso, o modelo √© interpret√°vel: os **coeficientes** indicam a import√¢ncia (positiva ou negativa) de cada vari√°vel para a predi√ß√£o.
