# KNN (K-Nearest Neighbors)

## ğŸ“Œ DescriÃ§Ã£o

O **KNN (K-Nearest Neighbors)** Ã© um algoritmo de aprendizado supervisionado usado tanto para **classificaÃ§Ã£o** quanto para **regressÃ£o**, sendo mais comum em tarefas de **classificaÃ§Ã£o**.

![Exemplo de KNN](https://miro.medium.com/v2/resize:fit:1358/0*jqxx3-dJqFjXD6FA)

Ele Ã© baseado na ideia de **proximidade**: para classificar um novo exemplo, o KNN analisa os `K` exemplos mais prÃ³ximos no conjunto de dados de treino e atribui a classe **mais comum** entre eles.

> ğŸ§  IntuiÃ§Ã£o: â€œDiga-me com quem andas, e eu te direi quem Ã©s.â€ â€“ Se os vizinhos mais prÃ³ximos sÃ£o da classe A, Ã© provÃ¡vel que o novo ponto tambÃ©m pertenÃ§a Ã  classe A.

---

## 1. ğŸ§­ Como Funciona?

1. Escolha um nÃºmero **K** de vizinhos (ex: 3, 5, 7...).
2. Calcule a **distÃ¢ncia** entre o ponto a ser classificado e todos os pontos do conjunto de treinamento (geralmente usando a **distÃ¢ncia Euclidiana**).
3. Selecione os **K vizinhos mais prÃ³ximos**.
4. Para **classificaÃ§Ã£o**:
   - FaÃ§a uma votaÃ§Ã£o entre as classes dos vizinhos.
   - A classe mais frequente Ã© atribuÃ­da ao novo ponto.
5. Para **regressÃ£o**:
   - A mÃ©dia dos valores dos vizinhos Ã© usada como prediÃ§Ã£o.

---

## 2. ğŸ“ DistÃ¢ncias Comuns

- **Euclidiana**: `sqrt((x1 - x2)^2 + (y1 - y2)^2)`
- **Manhattan**: `|x1 - x2| + |y1 - y2|`
- **Minkowski**: generaliza as anteriores
- **Cosseno**: Ãºtil para vetores em espaÃ§os de alta dimensÃ£o (ex: texto)

> âš ï¸ A escolha da **mÃ©trica de distÃ¢ncia** pode afetar bastante a performance do algoritmo.

---

## 3. ğŸ”§ HiperparÃ¢metros Importantes

| ParÃ¢metro      | DescriÃ§Ã£o |
|----------------|-----------|
| `n_neighbors`  | NÃºmero de vizinhos a considerar. Valores baixos tornam o modelo mais **sensÃ­vel ao ruÃ­do**. |
| `weights`      | Pode ser `"uniform"` (padrÃ£o) ou `"distance"` (mais peso para vizinhos mais prÃ³ximos). |
| `metric`       | Tipo de distÃ¢ncia usada (padrÃ£o: `"minkowski"`). |
| `p`            | ParÃ¢metro da mÃ©trica Minkowski (p=2 â†’ Euclidiana, p=1 â†’ Manhattan). |

---

## 4. ğŸ“Š AvaliaÃ§Ã£o e Performance

O KNN Ã© um **modelo preguiÃ§oso**: ele **nÃ£o aprende** um modelo explÃ­cito, apenas armazena os dados de treinamento e calcula tudo na fase de inferÃªncia.

ğŸ“‰ Pode ser **ineficiente com grandes volumes de dados**, pois precisa calcular distÃ¢ncias para todos os pontos durante a prediÃ§Ã£o.

âœ… MÃ©tricas comuns para avaliaÃ§Ã£o de classificaÃ§Ã£o:

- AcurÃ¡cia
- PrecisÃ£o / Recall
- F1-score
- Matriz de ConfusÃ£o

---

## ğŸ§  Dicas Finais

- Escolha **K Ã­mpar** para evitar empates em classificaÃ§Ã£o binÃ¡ria.
- O **escalonamento dos dados** Ã© essencial para o KNN.
- Para grandes bases de dados, considere o uso de estruturas como **KD-Trees** ou **Ball Trees** para melhorar a performance.
- KNN funciona melhor quando os dados possuem **baixa dimensionalidade** e **pouco ruÃ­do**.

---